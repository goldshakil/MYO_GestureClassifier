{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3c_EyBQAeVqG"
   },
   "source": [
    "\n",
    "\n",
    "## Gesture Classification with SEN-ResNet (Custom Resnet 26)\n",
    "\n",
    "Please change the CIFAR-10 data to your local images resized to 32*32*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4fHqJjYd-mo"
   },
   "outputs": [],
   "source": [
    "#Headers Definition\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import random\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "in8tjl-5ecJx"
   },
   "source": [
    "###Loading and splitting the data (Train/ Test/ Validate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": [
      "8a3d7c79319a4aa2afd45ecdc6b630c1",
      "788b713824ac4adfa89f89f78b1dba9d",
      "b5f915bb960843c08e7389e6d6694a01",
      "ce7cd28daa764cc8a3adeb54058c84f9",
      "1daf1f3101ec418a8a87838e4b56b750",
      "b94f43c2750243b7973adc581dbab705",
      "8ec46e7e7b3d4eadac30aaffc07250eb",
      "f6e1fa5a63fa450991b3bcc7d55d44e6"
     ]
    },
    "colab_type": "code",
    "id": "0IyOWHGwedGE",
    "outputId": "83ca307c-5eda-4e49-dd0d-46191e22a56e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3d7c79319a4aa2afd45ecdc6b630c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Transformations\n",
    "transform = transforms.Compose([     transforms.RandomCrop(32, padding=4), \n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                                     ]) #normalize each channel =>image = (image - mean) / std\n",
    "\n",
    "transform_test = transforms.Compose([ transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                                     ]) #normalize each channel =>image = (image - mean) / std\n",
    "\n",
    "                  \n",
    "\n",
    "\n",
    "#loading the dataset and preprocessing it\n",
    "# Change this to your local dataset and resize your data to 32*32*3 .........\n",
    "CIFAR_train= torchvision.datasets.CIFAR10(\"./data\",train=True, download=True, transform=transform) #Training Data\n",
    "CIFAR_test= torchvision.datasets.CIFAR10(\"./data\",train=False, download=True, transform=transform_test) #Testing Data\n",
    "\n",
    "#Create Validation Set (Stratify from SKLEARN can be used but CIFAR is very balanced so no need)\n",
    "indices = list(range(len(CIFAR_train)))\n",
    "seed=30\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(np.floor(0.9 * len(CIFAR_train)))\n",
    "tr_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "tr_sampler = SubsetRandomSampler(tr_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    " \n",
    "\n",
    "#How are we gonna iterate over the data?\n",
    "train_loader= torch.utils.data.DataLoader(CIFAR_train,batch_size=128,sampler=tr_sampler,num_workers=2) #batch_size : process the data in batches and make a better generalization\n",
    "valid_loader= torch.utils.data.DataLoader(CIFAR_train,batch_size=128,sampler=val_sampler,num_workers=2)\n",
    "test_loader= torch.utils.data.DataLoader(CIFAR_test,batch_size=128,shuffle=False,num_workers=2)  #shuffling the data makes a better generalization\n",
    "\n",
    "# data labels\n",
    "labels = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "#each data has 4 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qpu6l7ONelgv"
   },
   "source": [
    "###This cell is only for checking the visuals for one picture (You can skip this): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "htzhbUzwejqq",
    "outputId": "d23b5d72-dd79-4004-a7d7-082236ccf9ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n",
      "plane\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#every type you run this it is shuffled\n",
    "for data in train_loader:\n",
    "  print(data[0].shape)  # batch_size, # channels, #height, #width\n",
    "  break\n",
    "\n",
    "#Every data is a list of: 1.#(batch_size)images   2.#(batch_size)labels\n",
    "\n",
    "# show images\n",
    "plt.imshow(np.transpose(data[0][0], (1, 2, 0))) #replace 0 with 1 axis and 1 with 2 and 2 with 0  -> output: height,width ,channel\n",
    "plt.show\n",
    "print(labels[data[1][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XQOnCFO4tF85"
   },
   "source": [
    "### Sqeueeze and Excitation Block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zZ6Oq6ltFtN"
   },
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "  #Initialize your BasicBlock strucutre\n",
    "  def __init__(self, channel, reduction=4):\n",
    "    super().__init__()\n",
    "    self.avg_pool = nn.AdaptiveAvgPool2d(1) #Squeezing\n",
    "\n",
    "    #excitation\n",
    "    self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "  def forward(self, x):\n",
    "    #x is the output of the original residual block -> after finding weights scale and multiply\n",
    "    b, c, _, _ = x.size()\n",
    "    y = self.avg_pool(x).view(b, c)\n",
    "    y = self.fc(y).view(b, c, 1, 1) #rescale to image\n",
    "    return x * y.expand_as(x) #y is the weight of each feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVWNP-ajevOG"
   },
   "source": [
    "### Basic Block Definition (BottleNeck Block is not needed in ResNet-18):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oF9L82G2etgf"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "  #Initialize your BasicBlock strucutre\n",
    "  def __init__(self,in_channels,out_channels,stride=1):\n",
    "    super().__init__()\n",
    "\n",
    "    # Remember:\n",
    "        #Conv2d Parameters: input channels,output channels, size_kernel\n",
    "        #BatchNorm Goals: Normalize the outputs, parameters:(#output_channels/ node)\n",
    "    \n",
    "    #First Convolution: changes the dimensions of image according to the given parameters\n",
    "    self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1) #change channels   #reduce size to half if stride is set to two\n",
    "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    #Second Convolution: doesn't change the dimensions at all (ignore it in calculation)\n",
    "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    #SE Block\n",
    "    self.SE_Block = SEBlock(out_channels)\n",
    "\n",
    "    #This changes the input size to match the output size: #change channels   #reduce size to half if stride is set to two\n",
    "    self.input_changer = nn.Sequential()\n",
    "    if stride != 1  or in_channels != out_channels:\n",
    "      self.input_changer = nn.Sequential( nn.Conv2d(in_channels, out_channels, 1, stride=stride),nn.BatchNorm2d(out_channels))\n",
    "\n",
    "  def forward(self, x):\n",
    "    #x is original data\n",
    "    output=x\n",
    "    #Pass through the first convolution\n",
    "    output= F.relu(self.bn1(self.conv1(output)))  #conv -> normalize -> activate/threshold\n",
    "    #Pass through the first convolution\n",
    "    output= F.relu(self.bn2(self.conv2(output)))\n",
    "    #Multiply by SE_Block Weights\n",
    "    output=self.SE_Block(output)\n",
    "    #Add the input\n",
    "    output+=self.input_changer(x)\n",
    "    return F.relu(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "caa0jW2Sm2bS"
   },
   "source": [
    "### ResNet-26 Neural Network Definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GpMMg605mgUY"
   },
   "outputs": [],
   "source": [
    "class ResNet_18(nn.Module):\n",
    "  #initialize your network strucutre\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1) #3,32,32 -> 64,32,32\n",
    "    self.bn1 = nn.BatchNorm2d(64)\n",
    "    \n",
    "    \n",
    "    #First Block: Change channels   #Other Blocks: Don't change anything  #Last Block: Change dimesnsions using stride 2\n",
    "    self.conv2=nn.Sequential(BasicBlock(64,64,1),BasicBlock(64,64,1),BasicBlock(64,64,1)) #64,32,32 -> 64,32,32 ->64,32,32\n",
    "\n",
    "    self.conv3=nn.Sequential(BasicBlock(64,128,1),BasicBlock(128,128,1),BasicBlock(128,128,2)) #64,32,32 ->128,32,32 ->128,16,16\n",
    "    \n",
    "    self.conv4=nn.Sequential(BasicBlock(128,256,1),BasicBlock(256,256,1),BasicBlock(256,256,2)) #128,16,16 ->256,16,16 ->256,8,8\n",
    "    \n",
    "    self.conv5=nn.Sequential(BasicBlock(256,512,1),BasicBlock(512,512,1),BasicBlock(512,512,2)) #256,8,8 ->512,8,8 ->512,4,4\n",
    "\n",
    "    self.pool = nn.AvgPool2d(2, 2) #512,4,4 -> 512,2,2\n",
    "\n",
    "    self.fc=nn.Linear(512*2*2, 10)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x=F.relu(self.bn1(self.conv1(x))) #conv -> normalize -> activate/threshold\n",
    "\n",
    "    #BasicBlocks\n",
    "    x=self.conv2(x)\n",
    "    x=self.conv3(x)\n",
    "    x=self.conv4(x)\n",
    "    x=self.conv5(x)\n",
    "\n",
    "    x=self.pool(x)\n",
    "\n",
    "    #flatten\n",
    "    x=x.view(-1, 512*2*2)\n",
    "    x=self.fc(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndD1OLoD-tlb"
   },
   "source": [
    "###Driver Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "6yDG82jM0gFy",
    "outputId": "16c1c9a6-87d9-4d79-e88e-7a6d31cdf064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "This model is running on Tesla P4\n"
     ]
    }
   ],
   "source": [
    "# Select GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(\"This model is running on\" , torch.cuda.get_device_name())\n",
    "\n",
    "#Model\n",
    "net=ResNet_18().to(device)\n",
    "\n",
    "#Get adjustable parameters(weights) and optimize them \n",
    "optimizer=optim.Adam(net.parameters(),lr=0.001,weight_decay=0.0001) #weight decay is multiplied to weight to prevent them from growing too large\n",
    "\n",
    "#Error Function\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# Learning rate scheduler: adjusts learning rate as the epoch increases\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1) #Decays the learning rate by multiplyin by gamma every step_size epochs\n",
    "\n",
    "#How many times we pass our full data (the same data)\n",
    "total_epoch=50 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fvH7-Oc1_psN"
   },
   "source": [
    "###Training and Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "WVhiNaxV-wgu",
    "outputId": "48f93553-3d5c-47b5-d1f6-2a1f71a8112b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 5/50: Training Accuracy 0.7813111111111111 |  Training Loss 0.6260925026779826 || Validation Accuracy 0.7764 |  Validation Loss 0.6579059712588787\n",
      " Best validation so far 0.7764\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 10/50: Training Accuracy 0.8597777777777778 |  Training Loss 0.41012049453671684 || Validation Accuracy 0.8328 |  Validation Loss 0.5088349133729935\n",
      " Best validation so far 0.8328\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 15/50: Training Accuracy 0.8920666666666667 |  Training Loss 0.31536976887251844 || Validation Accuracy 0.8326 |  Validation Loss 0.5067954331636428\n",
      " Best validation so far 0.859\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 20/50: Training Accuracy 0.9142222222222223 |  Training Loss 0.24974737358702856 || Validation Accuracy 0.8822 |  Validation Loss 0.3592170745134354\n",
      " Best validation so far 0.8822\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 25/50: Training Accuracy 0.9707333333333333 |  Training Loss 0.08773289230355824 || Validation Accuracy 0.9218 |  Validation Loss 0.2494253132492304\n",
      " Best validation so far 0.9268\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 30/50: Training Accuracy 0.9809333333333333 |  Training Loss 0.05676496447730725 || Validation Accuracy 0.9286 |  Validation Loss 0.2531512301415205\n",
      " Best validation so far 0.9286\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 35/50: Training Accuracy 0.9869111111111111 |  Training Loss 0.03820297918123701 || Validation Accuracy 0.9258 |  Validation Loss 0.2784077350050211\n",
      " Best validation so far 0.9294\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 40/50: Training Accuracy 0.9903555555555555 |  Training Loss 0.02953683097174772 || Validation Accuracy 0.9266 |  Validation Loss 0.31279024109244347\n",
      " Best validation so far 0.93\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 45/50: Training Accuracy 0.9953777777777778 |  Training Loss 0.0153798282104121 || Validation Accuracy 0.9344 |  Validation Loss 0.260229071136564\n",
      " Best validation so far 0.9344\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      " Epoch 50/50: Training Accuracy 0.9964444444444445 |  Training Loss 0.012375432679122738 || Validation Accuracy 0.9314 |  Validation Loss 0.2948284761980176\n",
      " Best validation so far 0.9344\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc=0\n",
    "\n",
    "for cur_epoch in range(total_epoch):\n",
    "  train_correct=0\n",
    "  train_total=0\n",
    "  train_loss=0 #loss per epoch\n",
    "\n",
    "  valid_correct=0\n",
    "  valid_total=0\n",
    "  valid_loss=0 #loss per epoch\n",
    "  \n",
    "  net.train() #put the model in training mode\n",
    "  for data in train_loader:\n",
    "\n",
    "    #every data consits of (batch_size) images\n",
    "    X,y=data[0].to(device), data[1].to(device) #picture(X batch_size), label(X batch_size) -> #batch size comes first #note that the label here is a number which is index in labels list\n",
    "    \n",
    "    net.zero_grad()  \n",
    "    output = net(X)  \n",
    "    loss = criterion(output, y) #calculate the error/ loss for the that batch (data)\n",
    "\n",
    "    loss.backward()  #computes dloss/dw for every parameter w  (loss for every parameter)\n",
    "    optimizer.step() #update weights\n",
    "    train_loss+=loss.item()\n",
    "\n",
    "    #calculate how many right do you have in every training data until the end of all training datas\n",
    "    #output is Batch_size*10 tensor\n",
    "    for k, i in enumerate(output): # the output is batch_size* 10 tensor   # k is the index of the data # i the data itself\n",
    "        if torch.argmax(i) == y[k]: # in every row find the highest prediction index and compare it to y[k]\n",
    "                train_correct += 1\n",
    "        train_total += 1\n",
    "\n",
    "  exp_lr_scheduler.step() #learning rate adjustment\n",
    "  \n",
    "  net.eval() #put the model in evaluation mode\n",
    "  #validate for each epoch\n",
    "  with torch.no_grad(): # no gradient\n",
    "    for data in valid_loader:\n",
    "      X, y = data[0].to(device), data[1].to(device) # store the images in X and labels in y\n",
    "      output = net(X) \n",
    "      loss = criterion(output, y)\n",
    "\n",
    "      valid_loss += loss.item()\n",
    "\n",
    "      for k, i in enumerate(output): # the output is batch_size* 10 ARRAY\n",
    "          if torch.argmax(i) == y[k]: # in every row find the highest prediction and comprae its index\n",
    "              valid_correct += 1\n",
    "          valid_total += 1\n",
    "  \n",
    "  #if the model is better than the previous best store it\n",
    "  if((valid_correct/valid_total)>best_valid_acc):\n",
    "    best_valid_acc= (valid_correct/valid_total)\n",
    "    torch.save(net.state_dict(), \"./save_best.pth\") #save early stopping point\n",
    "\n",
    "  if((cur_epoch+1)%(total_epoch*0.1)==0):\n",
    "    print(' Epoch {}/{}: Training Accuracy {} |  Training Loss {} || Validation Accuracy {} |  Validation Loss {}'.format(cur_epoch+1, total_epoch, train_correct/train_total,train_loss/len(train_loader),valid_correct/valid_total,valid_loss/len(valid_loader))) #accuray for each epoch\n",
    "    print(' Best validation so far {}'.format(best_valid_acc))\n",
    "    print('-------------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQpwjAelNzO0"
   },
   "source": [
    "###Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jsOHugmi_vdL",
    "outputId": "3da11833-b6d8-444b-fba2-206db376332d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.931\n"
     ]
    }
   ],
   "source": [
    "#load the best validation accuracy model so far\n",
    "load_model = ResNet_18().to(device)\n",
    "load_model.load_state_dict(torch.load(\"./save_best.pth\")) \n",
    "\n",
    "load_model.eval()\n",
    "\n",
    "correct =0\n",
    "total=0\n",
    "with torch.no_grad(): # no gradient\n",
    "  for data in test_loader:\n",
    "      X, y = data[0].to(device), data[1].to(device) # store the images in X and labels in y\n",
    "      output = load_model(X) #send the 4 images\n",
    "      #print(output)\n",
    "      for k, i in enumerate(output): # the output is 4* 10 ARRAY\n",
    "          if torch.argmax(i) == y[k]: # in every row find the highest prediction and comprae its index\n",
    "              correct += 1\n",
    "          total += 1\n",
    "\n",
    "print(\"Test Accuracy: \", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpGUdg_IN4uT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework8-2017314461-Muhammad.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1daf1f3101ec418a8a87838e4b56b750": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "788b713824ac4adfa89f89f78b1dba9d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a3d7c79319a4aa2afd45ecdc6b630c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b5f915bb960843c08e7389e6d6694a01",
       "IPY_MODEL_ce7cd28daa764cc8a3adeb54058c84f9"
      ],
      "layout": "IPY_MODEL_788b713824ac4adfa89f89f78b1dba9d"
     }
    },
    "8ec46e7e7b3d4eadac30aaffc07250eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5f915bb960843c08e7389e6d6694a01": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b94f43c2750243b7973adc581dbab705",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1daf1f3101ec418a8a87838e4b56b750",
      "value": 1
     }
    },
    "b94f43c2750243b7973adc581dbab705": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce7cd28daa764cc8a3adeb54058c84f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6e1fa5a63fa450991b3bcc7d55d44e6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8ec46e7e7b3d4eadac30aaffc07250eb",
      "value": " 170500096/? [00:07&lt;00:00, 21833213.18it/s]"
     }
    },
    "f6e1fa5a63fa450991b3bcc7d55d44e6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
